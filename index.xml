<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rok&#39;s Projects</title>
    <link>https://rokuc.github.io/</link>
      <atom:link href="https://rokuc.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Rok&#39;s Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 16 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://rokuc.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Rok&#39;s Projects</title>
      <link>https://rokuc.github.io/</link>
    </image>
    
    <item>
      <title>Customer Lifetime Value - Dashboarding - Part 2</title>
      <link>https://rokuc.github.io/project/customer-lifetime-value-dashboard/</link>
      <pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://rokuc.github.io/project/customer-lifetime-value-dashboard/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project overview&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Customer Lifetime Value - Analysis - Part 1</title>
      <link>https://rokuc.github.io/project/customer-lifetime-value-analysis/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://rokuc.github.io/project/customer-lifetime-value-analysis/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project overview&lt;/h2&gt;
&lt;p&gt;With new products launching and a growing focus on boosting our company&amp;rsquo;s ROI, our team took on the challenge of determining our companies customer lifetime value (CLV). We brainstormed ways to calculate CLV and settled on two approaches. Both approaches used RFM segmented data.&lt;/p&gt;
&lt;p&gt;RFM stands for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recency: Time since their last transaction.&lt;/li&gt;
&lt;li&gt;Frequency: How often they transacted in a set period.&lt;/li&gt;
&lt;li&gt;Monetary: Average spending over their entire time with us.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-two-approaches&#34;&gt;The two approaches&lt;/h2&gt;
&lt;p&gt;We decided to go with an approach inspired by a research paper from 2005 titled &amp;ldquo;Counting Your Customers the Easy Way: An Alternative to the Pareto/NBD Model&amp;rdquo; by Fader et al. A Python implementation was already available, providing us with better and richer insights then our second method.&lt;/p&gt;
&lt;p&gt;The second method involved assessing how likely customers were to spend and predicting their future expenses. By multiplying these values, we got the customer&amp;rsquo;s value.&lt;/p&gt;
&lt;h2 id=&#34;project-output&#34;&gt;Project output&lt;/h2&gt;
&lt;p&gt;The approach employed by Fader et al. considers only two variables: Recency and Frequency. Based on these values, we can intuitive calculate the customer value.&lt;/p&gt;
&lt;p&gt;The model yields the repeat purchase frequency and the probability of observing the given number of purchases. The customer value is subsequently calculated by taking the product of the two together and further multiplying it by the individual&amp;rsquo;s spending habit.&lt;/p&gt;
&lt;p&gt;Final deliverable was a dashboard which could be used for audience activation. This will be discussed in &lt;a href=&#34;https://rokuc.github.io/project/customer-lifetime-value-dashboard/&#34;&gt;part 2&lt;/a&gt; of this article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An autonomous web crawler</title>
      <link>https://rokuc.github.io/project/e-store-crawler/</link>
      <pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://rokuc.github.io/project/e-store-crawler/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project overview&lt;/h2&gt;
&lt;p&gt;Our team developed a Dashboard for multiple stakeholders. While the users were happy with the dashboard, manual pipelines were time-consuming. To enhance efficiency, our team decided to create an automated crawler to directly populate a database with the same data previously stored as CSV files on SharePoint. The crawler was set to run daily through the Windows Task Scheduler which triggered batch scripts every morning.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Schema.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Challenges faced while working on the crawler:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoiding Frequent Crashes: Initial problem was preventing crashes from frequent clicks. We tackled it using try-except blocks and loops.&lt;/li&gt;
&lt;li&gt;Handling Dynamic Website Changes: Adapting to website changes was tough. We made it work by spotting elements through their inner text.&lt;/li&gt;
&lt;li&gt;Getting Crawler Status Updates: Knowing when the crawler ran or crashed was tricky. We nailed it by setting up automatic emails reporting on the latest date available in our dataset.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://rokuc.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://rokuc.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
