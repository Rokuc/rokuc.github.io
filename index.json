
    
    [{"authors":null,"categories":null,"content":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.\nThis page is divided into two sections: the projects section - where you’ll find summaries of the projects I worked on, and the posts section - where I deep-dive into more technical topics (thing’s I learned, code examples etc.).\nDownload my resumé .\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.","tags":null,"title":"Rok Jankovic","type":"authors"},{"authors":null,"categories":null,"content":"Project overview In this project we will be using selenium base to connect to a web page and collect some interesting data about real estate in Slovenia and connect that to geospatial data. We will conclude with a Demo visualization\nWe will touch upon four main topics:\nCreating a crawler using python, Mapping the data to Google BigQuery project and Postgresql as an alternative way of storring data, Preparing a basic visualization in LookerStudio. In a future article we will discuss the visualization of this data in a Django web Application using a Mapbox GLJS setup to plot the geospatial data..\nDisclaimer: The crawler discussed in this article is not created and should not be used for commercial purposes. It only serves the demonstrative purpose of this article.\nDelving into Selenium The packages we will be using are listed bellow.\nimport time from seleniumbase import Driver from selenium.webdriver.common.by import By import pandas as pd We will further explore some basic functions that we will be using during our entire project.\ndef find_element_by_attr(browser, tag, attr, value,multiple): xpath = f\u0026#34;//{tag}[@{attr}=\u0026#39;{value}\u0026#39;]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) def find_element_by_innertext(browser, tag, innertext,multiple): xpath = f\u0026#34;//{tag}[contains(text(),\u0026#39;{innertext}\u0026#39;)]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) def find_element_by_regex(browser, tag, attr, x_regx,multiple): xpath = f\u0026#34;//{tag}[contains(@{attr},\u0026#39;{x_regx}\u0026#39;)]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) def look_for_child_by_attribute(list_of_children, attr_lookup, wanted_attr): child = \u0026#39;\u0026#39; i = 0 while child != wanted_attr: child = list_of_children[i].get_attribute(attr_lookup) i=i+1 element = list_of_children[i] return element Let’s take the example bellow with a as the element of the html document:\n\u0026lt;a href=“rokuc.github.io”\u0026gt;My page\u0026lt;/a\u0026gt;\nhref is an attribute of element a and the value of href is “rokuc.github.io”. The inner text of this ellement is “My page”.\nThe find_element_by_ functions serve as facilitators to simplify the search of web elements and make the code more readable. The look_for_child_by_ code further helps loop through children of a parent element based on the childs attributes.\nLet’s fireup selenium with the following code.\nbrowser = Driver(uc=True) the uc (undetected-chromedriver) argument is a neat feture which evades bot-detection. This allows us to by pass cloudflare for example.\nIf you are ever looking to buy property in Slovenia, we will now start access the largest realestate portal in Slovenia.\ndef go_to_webpage(browser, webpage): browser.get(webpage) return browser def accept_cookies(browser): find_element_by_innertext(browser, \u0026#39;button\u0026#39;, \u0026#39;Dovoli vse \u0026#39;,multiple=False).click() return browser def start_browsing(browser): find_element_by_innertext(browser, \u0026#39;span\u0026#39;, \u0026#39;rezultate\u0026#39;,multiple=False).find_element(By.XPATH,\u0026#39;..\u0026#39;).click() return browser browser = go_to_webpage(browser, \u0026#39;https://www.nepremicnine.net/\u0026#39;) browser = accept_cookies(browser) start_browsing(browser) The sequence of functions above let’s us start searching the portal. We first accept all cookies and then move on to the page that includes all the listings by clicking the button that contains the following text “rezultate” (it’s it the “show results” button in Slovenian “Prikaži Rezultate”).\n","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666224000,"objectID":"64a58d9b6fde1764a7b5b8862892dbda","permalink":"https://rokuc.github.io/project/crawler/","publishdate":"2022-10-20T00:00:00Z","relpermalink":"/project/crawler/","section":"project","summary":"In this project we will download real estate data from a Slovenian page store the results in GBQ and visualize them with LookerBI.","tags":["Data Engineering","Recent"],"title":"An autonomous web crawler","type":"project"}]