
    
    [{"authors":null,"categories":null,"content":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.\nThis page is divided into two sections: the projects section - where you’ll find summaries of the projects I worked on, and the posts section - where I deep-dive into more technical topics (thing’s I learned, code examples etc.).\nDownload my resumé .\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.","tags":null,"title":"Rok Jankovic","type":"authors"},{"authors":null,"categories":null,"content":"Project overview We will use selenium base to connect to a web page and collect data about real estate prices in Slovenia, then merge it with geospatial data and finaly store it.\nThe two main topics we discuss are:\nCreating a crawler using python, Mapping the data to Google BigQuery. The data we will map to BigQuery can be visualized in LookerStudio as shown bellow.\nVisualization In a future article we will discuss the visualization of this data in a Django web application using a Mapbox GLJS setup to visualize the geospatial data and use postgresql as our storage.\nCreating a crawler using python Before we start crawling the web let’s look at an html code snippet from the webpage we will be crawling.\n\u0026lt;div class=\u0026#34;property-2 row column-sm property-label property-grid list-view no-sidebar\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-md-6 col-md-12 position-relative\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col-md-12 line\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col-md-6 col-md-12 position-relative\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col-md-12 line\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; The snippet starts with div instance which is a parent element. Every elemnt has a class attribute and is assigned a speciffic value. In our example attribute of parent element has value of “property-2 row column-sm property-label property-grid list-view no-sidebar”. Contrary to the code above every element can have multiple atributes. The indented elements stored within the parent div are called child elements. Finaly, inner text of each ellement could be written inplace of … the three dots.\nTo run our script we first need to import the packages we will use to crawl the web and define some basic functions.\nfrom seleniumbase import Driver from selenium.webdriver.common.by import By import pandas as pd import re def find_element_by_attr(browser, tag, attr, value, multiple): xpath = f\u0026#34;//{tag}[@{attr}=\u0026#39;{value}\u0026#39;]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) def find_element_by_innertext(browser, tag, innertext, multiple): xpath = f\u0026#34;//{tag}[contains(text(),\u0026#39;{innertext}\u0026#39;)]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) def find_element_by_regex(browser, tag, attr, x_regx, multiple): xpath = f\u0026#34;//{tag}[contains(@{attr},\u0026#39;{x_regx}\u0026#39;)]\u0026#34; if(multiple==True): return browser.find_elements(By.XPATH,xpath) else: return browser.find_element(By.XPATH,xpath) We can use the above find_element_by_ functions to simplify the search of web elements and make the code more readable for other users.\nTo start using the functions let’s first open the Chrome browser using python.\nbrowser = Driver(uc=True) The uc (undetected-chromedriver) argument is a neat feture of the Driver function which evades bot-detection, for example bypassing cloudflare, when surfing the web.\nTo start surfing the property listing portal with Selenium we will now move to the next code block. As a side note if you are ever looking to buy property in Slovenia, do not hesitate to visit Nepremicnine.net.\ndef go_to_webpage(browser, webpage): browser.get(webpage) return browser def accept_cookies(browser): find_element_by_innertext(browser, \u0026#39;button\u0026#39;, \u0026#39;Dovoli vse \u0026#39;,multiple=False).click() return browser def start_browsing(browser): find_element_by_innertext(browser, \u0026#39;span\u0026#39;, \u0026#39;rezultate\u0026#39;,multiple=False).find_element(By.XPATH,\u0026#39;..\u0026#39;).click() return browser browser = go_to_webpage(browser, \u0026#39;https://www.nepremicnine.net/\u0026#39;) browser = accept_cookies(browser) browser = start_browsing(browser) The sequence of functions above let’s us surf the portal and access the listings page. We first accept all cookies and then move on to listings page by clicking the button with “rezultate” written on it (it’s the “show results” button - in Slovenian “Prikaži Rezultate”).\nOnce the results are shown and we see all available listings on the page we can access them with Selenium by runing the code bellow and store the results in ad_list.\nad_list = find_element_by_regex(browser, \u0026#39;div\u0026#39;, \u0026#39;class\u0026#39;, \u0026#39;col-md-6 col-md-12 position-relative\u0026#39;, multiple = True) ad = ad_list[0] ad_description = ad.text.split(\u0026#39;\\n\u0026#39;) As we are not automating the crawler we will look at the first listing and store the price and property size information available in its description. If we were to further automate the process and go through every listing, we would have to create an additional for loop in our script.\nWithout this script extention we will collect the desired information from the listings page for a single ad and store it in a pandas dataframe.\ndf = pd.DataFrame(columns = [\u0026#39;Title\u0026#39;, \u0026#39;Size_m2\u0026#39;, \u0026#39;Price\u0026#39;, \u0026#39;Region\u0026#39;]) df_row = {\u0026#39;Title\u0026#39;:\u0026#39;\u0026#39;, \u0026#39;Size_m2\u0026#39;:\u0026#39;\u0026#39;, \u0026#39;Price\u0026#39;:\u0026#39;\u0026#39;, \u0026#39;Region\u0026#39;:\u0026#39;\u0026#39;} for element in range(0,len(ad_description)): df_row[\u0026#39;Title\u0026#39;] = \u0026#39;Ad n.1\u0026#39; if(\u0026#34;€\u0026#34; in str(ad_description[element])): df_row[\u0026#39;Price\u0026#39;] = ad_description[element] #ad_description[-1] if(\u0026#34;m2\u0026#34; in str(ad_description[element]) and \u0026#34;Zemljišče: \u0026#34; not in str(ad_description[element])): df_row[\u0026#39;Size_m2\u0026#39;] = …","date":1668211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668211200,"objectID":"64a58d9b6fde1764a7b5b8862892dbda","permalink":"https://rokuc.github.io/project/crawler/","publishdate":"2022-11-12T00:00:00Z","relpermalink":"/project/crawler/","section":"project","summary":"In this project we will download real estate data from a Slovenian listings page, store the results in GBQ and visualize them with Looker Studio.","tags":["Data Engineering","Recent"],"title":"Collecting, Storing, and Visualizing Real Estate Data","type":"project"}]