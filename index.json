
    
    
    
    
    [{"authors":null,"categories":null,"content":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.\nThis page is divided into two sections: the projects section - where you’ll find summaries of the projects I worked on, and the posts section - where I deep-dive into more technical topics (thing’s I learned, code examples etc.).\nDownload my resumé .\n","date":1695340800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1695340800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"In my career, I’ve gained experience in data engineering, analysis, and PowerBI dashboarding. I’m always eager to find the best solutions to any challenge. When I’m not working on projects, I love traveling and diving.","tags":null,"title":"Rok Jankovic","type":"authors"},{"authors":["Rok Jankovic"],"categories":[],"content":"Is varient B better than variant A In this post we will perform a hypothesis test where we would like to answer how likely it is that the version B of an e-mail or our website that is shown to our customers is better then version A. We will do this by using the Beta distribution and a series of random samplings from the distributions with the use of Monte Carlo simulations.\nInitial setup Imagine we would like to add an image or button to our website or e-mail and test if it helps or hurts our conversion rate. In our case we will go with an e-mail test, however, this can easily be applied on a web environment. We decide that the two e-mail variants that we will implement are one with images (variant A) and one without them (variant B). We assume that we currently have 1000 subscribers to our email and the test will be shown to 500 subscribers.\nPrior beliefs “The prior” tells us how the probability of our belief (weak inclination towards variant B outperforming variant A) is distributed, before taking into account the information contained in the data being analyzed. In our case we will use the Beta distribution which helps us estimate the probability of an event. The Priors can be informative (blue), weekly informative (green), or uninformative (red) depending on how confident we are in our belief.\nAnalyzing the data After collecting some data we get the following results, obviously the data will change over time, but we will discuss the automation towards the end of this article.\nClicked Not Clicked Observed conversion rate Variant A 60 190 0.24 Variant B 83 167 0.33 In order to calculate the posterior distribution we will add our likelihood and prior belief. The parameters of the Beta distribution can be viewed as the number of successful and failed trials where $\\alpha$ represents the number of successes and $\\beta$ shows the number of failures.\n$$Beta(\\alpha_{post.}, \\beta_{post.}) = Beta(\\alpha_{prior} + \\alpha_{likelihood}, \\beta_{prior} + \\beta_{likelihood})$$ Answering our questions using Python In order to activate our data we will use the NumPy and SciPy library, however before we do that we should recreate the table above in a Pandas DataFrame.\nd = {\u0026#39;Clicked\u0026#39;: [60, 83], \u0026#39;Not_Clicked\u0026#39;: [190, 167]} df = pd.DataFrame(data=d) Once we have the data in the right format we can move on to drawing a few samples from our distribution and seeing the results.\nn_trials = 100000 prior_a = 3 prior_b = 7 a_samples = stats.beta(prior_a + df.loc[0,\u0026#39;Clicked\u0026#39;], prior_b + df.loc[0,\u0026#39;Not_Clicked\u0026#39;]).rvs(n_trials) b_samples = stats.beta(prior_a + df.loc[1,\u0026#39;Clicked\u0026#39;], prior_b + df.loc[1,\u0026#39;Not_Clicked\u0026#39;]).rvs(n_trials) np.sum(a_samples \u0026lt; b_samples)/n_trials In our case the result returns ~96%, this indicates that variant B will outperform variant A in 96 out of 100 trials. We can also visualize our results in a probability density plot where you can clearly see that the conversion rate of variant B is higher than that of variant B. Automation Automation is always a tricky subject, hence the workflow should be well understood by the engineer working on this. Here are some suggestions on how this analysis can be automated.\nStep 1: Create a connection to the your database. This can be done through the use of pyodbc library.\nStep 2: Create a destination and think of a way that the data will be delivered.\nStep 3: Create a batch file that can automatically be triggered by a task scheduler.\n","date":1695340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695340800,"objectID":"74b49f8e94b3266a3c2841f6ee97ac6e","permalink":"https://rokuc.github.io/post/bayesian-ab-test/","publishdate":"2023-09-22T00:00:00Z","relpermalink":"/post/bayesian-ab-test/","section":"post","summary":"In this post we will discuss a Bayesian hypothesis test. In addition we will  go over how one could potentially automate this test.","tags":["Python"],"title":"Bayesian A/B Test","type":"post"},{"authors":null,"categories":null,"content":"Project overview In Part 1 We discussed the analytical model that was used to calculate the customer value. Once we had a model we had to think about how it can be visualized and how to deliver audiences to a location from where it could have been activated. We created a PowerBI dashboard where we visualized the audiences and allowed users to export them based on the filters that they applied.\nImplementation Our BI model was based on a simple star schema with a fact table and connecting dimension tables.\nAfter configuring the backend of the dashboard we sat down with our stakeholders proposing some visualizations.\nWe then created a simple reverse ETL which allowed users to send a request to our database based on audience parameters they selected. Once the request was fulfilled the audience information was delivered to a server from where it could have been activated.\n","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"1bab9b926b404d20186ce4e05a2135db","permalink":"https://rokuc.github.io/project/customer-lifetime-value-dashboard/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/project/customer-lifetime-value-dashboard/","section":"project","summary":"In this project we wanted to present our customer value model to stakeholders and provide them with a simple way to speed up the audience activation process. Feel free to read a short summary about this project.","tags":["Data Engineering","Recent"],"title":"Customer Lifetime Value - Dashboarding - Part 2","type":"project"},{"authors":null,"categories":null,"content":"Project overview With new products launching and a growing focus on boosting our company’s ROI, our team took on the challenge of determining our companies customer lifetime value (CLV). We brainstormed ways to calculate CLV and settled on two approaches. Both approaches used RFM segmented data.\nRFM stands for:\nRecency: Time since their last transaction. Frequency: How often they transacted in a set period. Monetary: Average spending over their entire time with us. The two approaches We decided to go with an approach inspired by a research paper from 2005 titled “Counting Your Customers the Easy Way: An Alternative to the Pareto/NBD Model” by Fader et al. A Python implementation was already available, providing us with better and richer insights then our second method.\nThe second method involved assessing how likely customers were to spend and predicting their future expenses. By multiplying these values, we got the customer’s value.\nProject output The approach employed by Fader et al. considers only two variables: Recency and Frequency. Based on these values, we can intuitive calculate the customer value.\nThe model yields the repeat purchase frequency and the probability of observing the given number of purchases. The customer value is subsequently calculated by taking the product of the two together and further multiplying it by the individual’s spending habit.\nFinal deliverable was a dashboard which could be used for audience activation. This will be discussed in part 2 of this article.\n","date":1671580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671580800,"objectID":"8a04687780a688f88a748e936bbe3e1b","permalink":"https://rokuc.github.io/project/customer-lifetime-value-analysis/","publishdate":"2022-12-21T00:00:00Z","relpermalink":"/project/customer-lifetime-value-analysis/","section":"project","summary":"In this project we wanted to calculate customer lifetime value. Moreover we wanted the model to be visualized in a dashboard and allow colleagues to activate audiences based on selected parameters in the dashboard. Feel free to read a short summary about this project. In this article we will cover the analytics behind the project.","tags":["Data Analysis","Recent"],"title":"Customer Lifetime Value - Analysis - Part 1","type":"project"},{"authors":null,"categories":null,"content":"Project overview Our team developed a Dashboard for multiple stakeholders. While the users were happy with the dashboard, manual pipelines were time-consuming. To enhance efficiency, our team decided to create an automated crawler to directly populate a database with the same data previously stored as CSV files on SharePoint. The crawler was set to run daily through the Windows Task Scheduler which triggered batch scripts every morning.\nChallenges faced while working on the crawler:\nAvoiding Frequent Crashes: Initial problem was preventing crashes from frequent clicks. We tackled it using try-except blocks and loops. Handling Dynamic Website Changes: Adapting to website changes was tough. We made it work by spotting elements through their inner text. Getting Crawler Status Updates: Knowing when the crawler ran or crashed was tricky. We nailed it by setting up automatic emails reporting on the latest date available in our dataset. ","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"717ed199343f1dfe291e2f3cda6dc3b1","permalink":"https://rokuc.github.io/project/e-store-crawler/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/project/e-store-crawler/","section":"project","summary":"In this project I supported our team in the implementation of an autonomous web crawler. First we collected data from the web. After collecting and transforming the data the data was mapped to a database. Feel free to read a short summary about this project.","tags":["Data Engineering","Recent"],"title":"An autonomous web crawler","type":"project"}]